As discussed over the phone, the next step in the hiring process is a take home test which will be reviewed in your first-round interview. Please only use your skillset and knowledge to complete this as there will be a proctored intake system in place to ensure that no other resources were used. ðŸ˜Š please complete and return ASAP, but donâ€™t rush as we want your best work to show through! If you have any questions, please let me know, I am available by phone. Thank you! 
 
You are asked to create an ETL process to inject a big stock pricing data into a database with limited a resource(in terms of memory and CPU).  The data is in 10 csv files of total size  100GB in random order,  and your ETL process will be run in a server with memory 32G and 4CPUs, and free disk space 100GB. The data structure of the data is in the following format: date:: datetime(%Y-%m-%d), id::int, price::float, trade_volume::int. The price values will be saved into a price table which has the following structure: date, stk_001,stk_002, stk_003,â€¦,stk_200. Assume there are 200 stocks in the data and the ids for the stocks are from 1 to 200. And save the trade_volume to a volume table with the following column structure: date, stk_001, stk_002, â€¦, stk_200. Find a most efficient solution in terms of run time and resources.
Calculate stock returns using above data.